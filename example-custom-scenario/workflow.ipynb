{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da46cc3a-c257-4274-a79e-09e81b5a616e",
   "metadata": {},
   "source": [
    "# Streaming a custom scenario: An example with the comma2k19 dataset\n",
    "This notebook shows an example of the workflow to reuse the semantic player presented in this repository with a custom scenario.\n",
    "\n",
    "### Overview of the example comma2k19 dataset\n",
    "As a demonstration, we use the public [comma2k19 dataset](https://github.com/commaai/comma2k19).\n",
    "It is a dataset that contains multiple journeys (aka., routes) in a commute are in California, United States.\n",
    "The dataset has the following structure:\n",
    "```\n",
    "Dataset_chunk_n\n",
    "|\n",
    "+-- route_id (dongle_id|start_time)\n",
    "    |\n",
    "    +-- segment_number\n",
    "        |\n",
    "        +-- preview.png (first frame video)\n",
    "        +-- raw_log.bz2 (raw capnp log, can be read with openpilot-tools: logreader)\n",
    "        +-- video.hevc (video file, can be read with openpilot-tools: framereader)\n",
    "        +-- processed_log/ (processed logs as numpy arrays, see format for details)\n",
    "        +-- global_pos/ (global poses of camera as numpy arrays, see format for details)\n",
    "```\n",
    "\n",
    "In this example, we focus on the `processed_log/` folder.\n",
    "This folder has several features available. \n",
    "According to its documentation, we have the following:\n",
    "```\n",
    "processed_log\n",
    "|\n",
    "+--IMU ([forward, right, down])\n",
    "|  |\n",
    "|  +--acceleration: (m^2/s)\n",
    "|  +--gyro_uncalibrated (rad/s)\n",
    "|  +--gyro_bias: android gyro bias estimate (rad/s)\n",
    "|  +--gyro: with android bias correction (rad/s)\n",
    "|  +--magnetic_uncalibrated: (T)\n",
    "|  +--magnetic: with android calibration(T)\n",
    "|\n",
    "+--CAN data:\n",
    "|  |\n",
    "|  +--car_speed (m/s)\n",
    "|  +--steering_angle (deg)\n",
    "|  +--wheel_speeds: [front_left, front_right, rear_left, rear_right] (m/s)\n",
    "|  +--radar: [forward distance (m),\n",
    "|  |          left distance (m),\n",
    "|  |          relative speed (m/s),\n",
    "|  |          nan,\n",
    "|  |          nan,\n",
    "|  |          address,\n",
    "|  |          new_track (bool)]\n",
    "|  +--raw CAN: This not stored as a value array but as three seperate arrays [src, address, data]\n",
    "|\n",
    "+--GNSS\n",
    "   |\n",
    "   +--live_gnss_qcom: [latitude (deg),\n",
    "   |                   longitude (deg),\n",
    "   |                   speed (m/s),\n",
    "   |                   utc_timestamp (s),\n",
    "   |                   altitude (m),\n",
    "   |                   bearing (deg)]\n",
    "   +--live_gnss_ublox: [latitude (deg),\n",
    "   |                    longitude (deg),\n",
    "   |                    speed (m/s),\n",
    "   |                    utc_timestamp (s),\n",
    "   |                    altitude (m),\n",
    "   |                    bearing (deg)]\n",
    "   |\n",
    "   +--raw_gnss_qcom: every row represents a measurement\n",
    "   |                 of 1 sattelite at 1 epoch can easily\n",
    "   |                 be manipulated with laika.\n",
    "   |                 [prn (nmea_id, see laika),\n",
    "   |                  week of gps_time of reception (gps_week),\n",
    "   |                  time pf week of gps_time of reception (s),\n",
    "   |                  nan,\n",
    "   |                  pseudorange (m),\n",
    "   |                  pseudorange_std (m),\n",
    "   |                  pseudorange_rate (m/s),\n",
    "   |                  pseudorange_rate_std (m/s)]\n",
    "   +--raw_gnss_ublox: every row represents a measurement\n",
    "                      of 1 sattelite at 1 epoch can easily\n",
    "                      be manipulated with laika.\n",
    "                      [prn (nmea_id, see laika),\n",
    "                       week of gps_time of reception (gps_week),\n",
    "                       time pf week of gps_time of reception (s),\n",
    "                       GLONASS channel number (-7..6) nan if not GLONASS,\n",
    "                       pseudorange (m),\n",
    "                       pseudorange_std (m),\n",
    "                       pseudorange_rate (m/s),\n",
    "                       pseudorange_rate_std (m/s)`\n",
    "```\n",
    "In this example, and to keep the demostration clear, we show the workflow for the features `car_speed (m/s)`and `steering_angle (deg)`. \n",
    "However, it could be easily extended by selecting any other feature.\n",
    "\n",
    "### Overview of the workflow\n",
    "The suggested workflow is illustrated in the following figure:\n",
    "\n",
    "![Image](figures/flow_for_custom_scenario.png)\n",
    "\n",
    "In the figure, we can distinguish three main parts, which are detailed in the rest of this notebook:\n",
    "* [(a) Dataset preparation.](#preparation) Preprocess and transform the dataset into a common shape/structure (e.g., a pandas `DataFrame`). This step is required when we work with datasets that have been previously stored. In case of working with data streams (e.g., observing a real sensor on real time), this step may be skipped.\n",
    "* [(b) Semantic annotation.](#semantify) Semantically annotate the record(s) of the dataset/stream (e.g., obtaining its corresponding RDF graph data).\n",
    "* [(c) Stream it out.](#stream_it_out) Broadcasting the semantic version of the record(s) sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b2ce4-eb66-45c6-95c8-ed14a0e10992",
   "metadata": {},
   "source": [
    "## (a) Dataset preparation <a id='preparation'></a>\n",
    "\n",
    "\n",
    "### Dependencies and reference paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aace562b-32ee-4bb2-89bc-ca48f267ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b61744-e5a7-468e-8e89-4febb4b4909d",
   "metadata": {},
   "source": [
    "> **Note:** Make sure you provide the proper path to the dataset, which must match the location where you stored it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffad57f8-8377-47e2-b24b-120b0d506a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The segments available for the journey b0c9d2329ad1606b|2018-08-24--18-40-57 are:\n",
      " [4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "journey_name = 'b0c9d2329ad1606b|2018-08-24--18-40-57'\n",
    "path_to_datasets = os.path.dirname(os.path.dirname(os.getcwd()))  # replace this with your own path\n",
    "journey_path = f'{path_to_datasets}/datasets/comma2k19/Chunk_2/{journey_name}/'\n",
    "segments_numbers = sorted([int(d.name) for d in os.scandir(journey_path) if d.is_dir()])  # get all the available segment numbers\n",
    "print(f'The segments available for the journey {journey_name} are:\\n {segments_numbers}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d49a9b4-cfd4-4fb5-95cc-70d9bb7eb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one segment\n",
    "segment_num = 9\n",
    "segment_path = f'/Users/q471483/Developer/datasets/comma2k19/Chunk_2/{journey_name}/{segment_num}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf751c07-810f-4584-bd2a-05b62f0998ef",
   "metadata": {},
   "source": [
    "### Feature selection and mapping reference to ontology concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9574e190-b153-4d17-b274-e0a215c7424e",
   "metadata": {},
   "source": [
    "Let us define the mapping (according to the documentation of the dataset) to the corresponding ontology concepts.\n",
    "In this example, we use the [Vehicle Signal Specification Ontology (VSSo)](https://github.com/w3c/vsso) as the reference for the instantiation of the data points.\n",
    "Thus, the names assigned in the dictionary match the VSSo concept names.\n",
    "> **Note:** The VSSo ontology is under development and you might encounter differences between the names used here and the ones described in the ontology. If any missmatch is found, please raise an issue on GitHub to update the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a14f45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Feature:\n",
    "    \"\"\"Class for characterizing a feature of the dataset.\"\"\"\n",
    "    name: str\n",
    "    dtype: str\n",
    "    path: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7745f02-fcf8-4949-a17c-167d257feab1",
   "metadata": {},
   "source": [
    "Use names of the VSSo concepts to facilitate the instantiation of concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de0dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping dictionary for the names of the features\n",
    "column_names_mapping = {\n",
    "    # Used in this example\n",
    "    'speed':'Speed',\n",
    "    'steering_angle':'SteeringWheelAngle',\n",
    "    \n",
    "    # Others: Modify/Extend it as needed\n",
    "    'radar_value_0':'radar_forward_distance',\n",
    "    'radar_value_1':'radar_left_distance',\n",
    "    'radar_value_2':'radar_relative_speed',\n",
    "    'radar_value_5':'radar_address',\n",
    "    'radar_value_6':'radar_new_track',\n",
    "    'wheel_speed_value_0':'wheel_speed_front_left',\n",
    "    'wheel_speed_value_1':'wheel_speed_front_right',\n",
    "    'wheel_speed_value_2':'wheel_speed_rear_left',\n",
    "    'wheel_speed_value_3':'wheel_speed_rear_right',\n",
    "    'live_gnss_ublox_value_0':'CurrentPositionLatitude',\n",
    "    'live_gnss_ublox_value_1':'CurrentPositionLongitude',\n",
    "    'live_gnss_ublox_value_2':'CurrentPositionSpeed',\n",
    "    'live_gnss_ublox_value_3':'CurrentPosition_utc_timestamp',\n",
    "    'live_gnss_ublox_value_4':'CurrentPositionAltitude',\n",
    "    'live_gnss_ublox_value_5':'CurrentPositionBearing',\n",
    "    'accelerometer_value_0':'AccelerationLongitudinal',\n",
    "    'accelerometer_value_1':'AccelerationLateral',\n",
    "    'accelerometer_value_2':'AccelerationVertical'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dbb8626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_of_interest(segment_path):\n",
    "    \"\"\"Returns a list with the features of interest (Intances of the dataclass Feature) for the giving segment path.\"\"\"\n",
    "\n",
    "    # Used in this example\n",
    "    \n",
    "    # speed\n",
    "    speed = Feature(name='speed', \n",
    "                    dtype=float, \n",
    "                    path=f'{segment_path}/processed_log/CAN/speed/')\n",
    "\n",
    "    # steering_wheel_angle\n",
    "    steering_angle = Feature(name='steering_angle',\n",
    "                             dtype='float64',\n",
    "                             path=f'{segment_path}/processed_log/CAN/steering_angle/')\n",
    "    \n",
    "    # Others: Modify/Extend it as needed\n",
    "\n",
    "    # radar\n",
    "    radar = Feature(name='radar',\n",
    "                    dtype='float64',\n",
    "                    path=f'{segment_path}/processed_log/CAN/radar/')\n",
    "\n",
    "    # wheel_speed\n",
    "    wheel_speed = Feature(name='wheel_speed',\n",
    "                          dtype='float64',\n",
    "                          path=f'{segment_path}/processed_log/CAN/wheel_speed/')\n",
    "\n",
    "    # live_gnss_ublox\n",
    "    live_gnss_ublox = Feature(name='live_gnss_ublox',\n",
    "                             dtype='float64',\n",
    "                             path=f'{segment_path}/processed_log/GNSS/live_gnss_ublox/')\n",
    "    \n",
    "    # accelerometer\n",
    "    accelerometer = Feature(name='accelerometer',\n",
    "                            dtype='float64',\n",
    "                            path=f'{segment_path}/processed_log/IMU/accelerometer/')\n",
    "\n",
    "    return [speed, steering_angle]  # select here the desired features to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f5e75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_from_df(index_name, feat_name, df):\n",
    "    \"\"\"Returns the selected feature as pandas DataFrame from the given DataFrame.\"\"\"\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df[[index_name, feat_name]]  # select feature of interest\n",
    "    df.set_index(index_name, inplace=True)\n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb7089b8-cf93-4218-b14b-51d68c7e8233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features from the dataset:\n",
      "speed\n",
      "steering_angle\n"
     ]
    }
   ],
   "source": [
    "features_of_interest = get_features_of_interest(segment_path)\n",
    "print('Selected features from the dataset:')\n",
    "for f in features_of_interest:\n",
    "    print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0f7bbf-8526-4678-805b-c57cfb18ad4d",
   "metadata": {},
   "source": [
    "### Load data of the selected features into one DataFrame with time as index\n",
    "Now, let's load the data associated to each of those features into a pandas DataFrame.\n",
    "Then, put all the DataFrames in a list called `all_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef38b253-208d-4d6d-ae6c-cf680626e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   speed\n",
      "105521.459948  27.577778\n",
      "105521.485107  27.563889\n",
      "105521.498111  27.537500\n",
      "105521.508899  27.536111\n",
      "105521.515962  27.552778\n",
      "               steering_angle\n",
      "105521.475753            -0.8\n",
      "105521.492700            -0.9\n",
      "105521.508846            -0.9\n",
      "105521.514293            -0.9\n",
      "105521.526352            -0.9\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "\n",
    "# Iterate over the desired features\n",
    "for f in features_of_interest:\n",
    "\n",
    "    # Load times and values of the current feature into numpy arrays\n",
    "    times = np.load(f.path+'t')\n",
    "    values = np.squeeze(np.load(f.path+'value'))\n",
    "    \n",
    "    # Initialize a DataFrame with the numpy arrays\n",
    "    current_df = pd.DataFrame()\n",
    "    \n",
    "    if len(values.shape) == 1:\n",
    "        # Use the feature name directly when only one variable is available\n",
    "        current_df = pd.DataFrame(data=values.tolist(), index=times, columns=[f.name])\n",
    "    else:\n",
    "        # Add numeration to the feature name when multiple variables are available\n",
    "        current_df = pd.DataFrame(data=values.tolist(), index=times, columns=[f'{f.name}_value_{c}' for c in range(len(values[0]))])\n",
    "\n",
    "    print(current_df.head(5))\n",
    "    \n",
    "    # Delete columns where all values are null\n",
    "    current_df.dropna(how='all', axis=1, inplace=True)    \n",
    "        \n",
    "    # Parse the index timestamps as DateTime\n",
    "    current_df.index = pd.to_datetime(current_df.index, unit='s')  # dataset documentation says the unit is seconds\n",
    "    current_df.index.name = 'datetime'\n",
    "    \n",
    "    # Drop rows where an index is repeated to avoid conflicts while resampling\n",
    "    current_df = current_df[~current_df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Resample to the DataFrame with ffill method\n",
    "    current_df.sort_index(inplace=True)\n",
    "    current_df = current_df.resample('250ms').ffill()\n",
    "    \n",
    "    # Append the DataFrame to a list\n",
    "    all_features.append(current_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977b23c-5b85-4c72-9dcf-ddf66ee71cad",
   "metadata": {},
   "source": [
    "For example, one of the features looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ad1c4f-604e-49c7-9563-2e914fa5b31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1970-01-02 05:18:41.250</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02 05:18:41.500</th>\n",
       "      <td>27.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02 05:18:41.750</th>\n",
       "      <td>27.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02 05:18:42.000</th>\n",
       "      <td>27.554167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970-01-02 05:18:42.250</th>\n",
       "      <td>27.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             speed\n",
       "datetime                          \n",
       "1970-01-02 05:18:41.250        NaN\n",
       "1970-01-02 05:18:41.500  27.537500\n",
       "1970-01-02 05:18:41.750  27.622222\n",
       "1970-01-02 05:18:42.000  27.554167\n",
       "1970-01-02 05:18:42.250  27.625000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5108311-b1bf-408c-9fd7-3d95cf6cc0ee",
   "metadata": {},
   "source": [
    "The `all_features` variable has a list of DataFrames corresponding to the selected features. \n",
    "It is now time to merge them all into one `segment_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68b723e6-0ff6-4d3e-8144-2eabb0ffe5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all features of the segment into one DataFrame\n",
    "segment_df = pd.DataFrame()\n",
    "for f_df in all_features:\n",
    "    if segment_df.empty:\n",
    "        segment_df = f_df\n",
    "    else:\n",
    "        segment_df = segment_df.join(f_df, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c630010c-81b3-4898-9b81-ebf17db134b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Speed</th>\n",
       "      <th>SteeringWheelAngle</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1970-01-02 05:18:41.500</td>\n",
       "      <td>27.537500</td>\n",
       "      <td>-0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970-01-02 05:18:41.750</td>\n",
       "      <td>27.622222</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970-01-02 05:18:42.000</td>\n",
       "      <td>27.554167</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1970-01-02 05:18:42.250</td>\n",
       "      <td>27.625000</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1970-01-02 05:18:42.500</td>\n",
       "      <td>27.640278</td>\n",
       "      <td>-1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  datetime      Speed  SteeringWheelAngle\n",
       "id                                                       \n",
       "0  1970-01-02 05:18:41.500  27.537500                -0.9\n",
       "1  1970-01-02 05:18:41.750  27.622222                -0.5\n",
       "2  1970-01-02 05:18:42.000  27.554167                -0.4\n",
       "3  1970-01-02 05:18:42.250  27.625000                -0.5\n",
       "4  1970-01-02 05:18:42.500  27.640278                -1.2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the name of the columns (if needed) according to the column_names_mapping dict\n",
    "segment_df.rename(columns=column_names_mapping, inplace=True)\n",
    "\n",
    "# Drop null values\n",
    "segment_df.dropna(inplace=True)\n",
    "\n",
    "segment_df.reset_index(inplace=True)\n",
    "\n",
    "segment_df.index.name = 'id'\n",
    "\n",
    "segment_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35ca1d-8e43-4041-9998-e42b72a47f89",
   "metadata": {},
   "source": [
    "We can observe that the result of the data preprocessing ended up with a table-like structure.\n",
    "One can also decide not to resample the data.\n",
    "In that case, we may face several null values bacause, in practice, some observations do not occur at the exact same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd023c90-c4be-47e0-80c1-cc933d242c28",
   "metadata": {},
   "source": [
    "## (b) Semantic annotation <a id='semantify'></a>\n",
    "The data of one segment is now in a table-like structure.\n",
    "The next step is to obtain its semantic version (e.g., with RDF triples).\n",
    "In a streaming setup, we do that by iterating over the rows of the DataFrame that are sorted by time.\n",
    "Then, a mapping function is applied. \n",
    "Such a mapping function requires a certain schema defined from a semantic model.\n",
    "For instance, an schema for the annotation of vehicle properties is illustrated below:\n",
    "\n",
    "<img src=\"figures/vsso_sosa_schema_simplified.png\" width=\"500\">\n",
    "\n",
    "\n",
    "For example, the following RML rules map the observations of the speed and steering wheel angle:\n",
    "```turtle\n",
    "@prefix rr: <http://www.w3.org/ns/r2rml#> .\n",
    "@prefix rml: <http://semweb.mmlab.be/ns/rml#> .\n",
    "@prefix ql: <http://semweb.mmlab.be/ns/ql#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "@prefix schema: <http://schema.org/> .\n",
    "@prefix sem-rc: <http://stream-reasoning-challenge.org/>.\n",
    "@prefix sem-speed-obs: <http://stream-reasoning-challenge/vehicle/speed/observation/>.\n",
    "@prefix indv: <http://stream-reasoning-challenge.org/indv/>.\n",
    "@prefix vsso: <https://github.com/w3c/vsso#> .\n",
    "@prefix sosa: <http://www.w3.org/ns/sosa/> .\n",
    "\n",
    "#Dataset\n",
    "sem-rc:speed a rr:TriplesMap ;\n",
    "    rml:logicalSource\n",
    "        [\n",
    "            rml:source \"comma.csv\" ;\n",
    "            rml:referenceFormulation ql:CSV\n",
    "        ] ;\n",
    "#subject\n",
    "    rr:subjectMap\n",
    "        [\n",
    "            rr:template \"http://stream-reasoning-challenge/vehicle/speed/observation/{id}\" ;\n",
    "            rr:class sosa:Observation\n",
    "        ] ;\n",
    "\n",
    "    rr:predicateObjectMap\n",
    "        [\n",
    "            rr:predicateMap [ rr:constant sosa:observedProperty ] ;\n",
    "            rr:objectMap\n",
    "                [\n",
    "                    rr:constant indv:Speed\n",
    "                ]\n",
    "        ] ;\n",
    "\n",
    "    rr:predicateObjectMap\n",
    "        [\n",
    "            rr:predicate sosa:hasSimpleResult;\n",
    "            rr:objectMap\n",
    "                [\n",
    "                    rml:reference \"speed\" ;\n",
    "                    rr:datatype xsd:float\n",
    "                ]\n",
    "        ] ;\n",
    "\n",
    "    rr:predicateObjectMap\n",
    "        [\n",
    "            rr:predicate sosa:resultTime;\n",
    "            rr:objectMap\n",
    "                [\n",
    "                    rml:reference \"datetime\" ;\n",
    "                    rr:datatype xsd:dateTime\n",
    "                ]\n",
    "        ] ;\n",
    ".\n",
    "\n",
    "#Dataset\n",
    "sem-rc:steering a rr:TriplesMap ;\n",
    "    rml:logicalSource\n",
    "        [\n",
    "            rml:source \"comma.csv\" ;\n",
    "            rml:referenceFormulation ql:CSV\n",
    "        ] ;\n",
    "#subject\n",
    "    rr:subjectMap\n",
    "        [\n",
    "            rr:template \"http://stream-reasoning-challenge/vehicle/steering/observation/{id}\" ;\n",
    "            rr:class sosa:Observation\n",
    "        ] ;\n",
    "\n",
    "    rr:predicateObjectMap\n",
    "        [\n",
    "            rr:predicateMap [ rr:constant sosa:observedProperty ] ;\n",
    "            rr:objectMap\n",
    "                [\n",
    "                    rr:constant indv:SteeringWheelAngle\n",
    "                ]\n",
    "        ] ;\n",
    "\n",
    "    rr:predicateObjectMap\n",
    "        [\n",
    "            rr:predicate sosa:hasSimpleResult;\n",
    "            rr:objectMap\n",
    "                [\n",
    "                    rml:reference \"steering\" ;\n",
    "                    rr:datatype xsd:float\n",
    "                ]\n",
    "        ] ;\n",
    "\n",
    "    rr:predicateObjectMap\n",
    "        [\n",
    "            rr:predicate sosa:resultTime;\n",
    "            rr:objectMap\n",
    "                [\n",
    "                    rml:reference \"datetime\" ;\n",
    "                    rr:datatype xsd:dateTime\n",
    "                ]\n",
    "        ] ;\n",
    ".\n",
    "\n",
    "```\n",
    "\n",
    "The associated background knowledge would be the ontology from where the individuals are instantiated:\n",
    "```turtle\n",
    "@prefix indv: <http://stream-reasoning-challenge.org/indv/>.\n",
    "@prefix vsso: <https://github.com/w3c/vsso#> .\n",
    "\n",
    "indv:Speed a vsso:Speed .\n",
    "indv:SteeringWheelAngle a vsso:SteeringWheelAngle .\n",
    "```\n",
    "\n",
    "The resulting RDF data (at time `1970-01-02 05:18:41.750`) looks like the following: \n",
    "```turtle\n",
    "@prefix sosa: <http://www.w3.org/ns/sosa/> .\n",
    "@prefix vsso: <https://github.com/w3c/vsso#> .\n",
    "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
    "\n",
    "<http://stream-reasoning-challenge/vehicle/speed/observation/1> a sosa:Observation;\n",
    "  sosa:hasSimpleResult \"27.622222222222224\"^^xsd:float;\n",
    "  sosa:observedProperty vsso:Speed;\n",
    "  sosa:resultTime \"1970-01-02 05:18:41.750\"^^xsd:dateTime .\n",
    "\n",
    "<http://stream-reasoning-challenge/vehicle/steering/observation/1> a sosa:Observation;\n",
    "  sosa:hasSimpleResult \"-0.5\"^^xsd:float;\n",
    "  sosa:observedProperty vsso:SteeringWheelAngle;\n",
    "  sosa:resultTime \"1970-01-02 05:18:41.750\"^^xsd:dateTime .\n",
    "\n",
    "```\n",
    "\n",
    "This data will be broadcasted in a message by a custom player.\n",
    "\n",
    "> **Note: ** Please refer to the `semantization/` folder to see the corresponding mapping to `RDF` data using the RDF Mapping Language (RML)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1179d-bb55-43ae-9815-f4988a605d3e",
   "metadata": {},
   "source": [
    "## (c) Message broadcast <a id='stream_it_out'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949e07a-edf6-4d0b-865a-9553936eb28c",
   "metadata": {},
   "source": [
    "To complete the workflow, we need to broadcast the message with the current semantic data.\n",
    "For that purpose, we can use as a reference the [`simple_player.py` script](https://github.com/patrik999/stream-reasoning-challenge/blob/master/src/simple_player.py) provided in the repository.\n",
    "A custom player for our semantic data would look like the following:\n",
    "\n",
    "\n",
    "```python\n",
    "import getopt, sys\n",
    "import optparse\n",
    "import time\n",
    "from abstract_player import AbstractPlayer \n",
    "\n",
    "class CustomPlayer(AbstractPlayer):\n",
    " \n",
    "    def init(self, stream_id, template_id):  # __init__\n",
    "        self.streamID = stream_id\n",
    "        self.templateID = template_id\n",
    "\n",
    "    def start(self, freq_in_ms, replay):\n",
    "        '''Modify this function according to your needs.'''\n",
    "        \n",
    "        self.frequency = freq_in_ms\n",
    "        \n",
    "        # ------------------------------\n",
    "        # TODO Implement here your own functionality\n",
    "        # ------------------------------\n",
    "        \n",
    "        # Load the dataset\n",
    "        dataset = load_the_dataset()  # TODO use your own function/code\n",
    "        \n",
    "        # Prepare the dataset (as explained in this notebook)\n",
    "        df = prepare_dataset(dataset)  # TODO use your own function/code\n",
    "        \n",
    "        # Iterate over the rows of the data frame\n",
    "        for row in df.iterrows():\n",
    "            \n",
    "            # Semantify the current row of records\n",
    "            msgText = semantify_this_row(row_of_records)  # TODO use your own function/code\n",
    "            yield msgText\n",
    "            time.sleep(self.frequency/1000.0)\n",
    "        \n",
    "    def modify(self, freq_in_ms):\n",
    "        self.frequency = freq_in_ms\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
